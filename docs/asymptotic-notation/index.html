<!DOCTYPE html>
<!--[if lt IE 7]>      <html lang="en-us" class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]>         <html lang="en-us" class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]>         <html lang="en-us" class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html lang="en-us" class="no-js"> <!--<![endif]-->
    <head>
      <!-- Google tag (gtag.js) -->
      <script async src="https://www.googletagmanager.com/gtag/js?id=G-WXC8R75DEN"></script>
      <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-WXC8R75DEN');
      </script>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <meta http-equiv="Content-Language" content="en-us">
        <!-- Use title if it's in the page YAML frontmatter -->
        <title>Learn X in Y Minutes: Scenic Programming Language Tours</title>
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <!-- Place favicon.ico and apple-touch-icon.png in the root directory -->

        <link rel="stylesheet" href="/css/index.css">

        <link rel="canonical" href="https://learnxinyminutes.com/docs/asymptotic-notation/">
        <script>
            var THEME_KEY = "lxiym_theme";
            function set_theme(theme) {
                var el = document.getElementsByTagName("html")[0];
                if (!el) {
                  return;
                }

                if (theme === "dark" ) {
                    el.className = "dark";
                } else {
                    el.className = "light";
                }

                localStorage.setItem(THEME_KEY, theme);
            }

            function load_theme() {
              var theme = localStorage.getItem(THEME_KEY);
              if (theme) {
                set_theme(theme);
              }
            }

            // Attempt immediate application of html style
            load_theme();

            // Backup: do it onload. Will flash, but better than nothing.
            window.addEventListener("load", function(){
              load_theme();
            });
        </script>
    </head>
    <body>
        <!--[if lt IE 7]>
            <p class="chromeframe">You are using an outdated browser. <a href="http://browsehappy.com/">Upgrade your browser today</a> or <a href="http://www.google.com/chromeframe/?redirect=true">install Google Chrome Frame</a> to better experience this site.</p>
        <![endif]-->

        <div class="container">
            <div class="share">
    <span class="sharemsg">
      <a href="https://twitter.com/intent/tweet?url=https%3A%2F%2Flearnxinyminutes.com%2Fdocs%2Fasymptotic-notation%2F&text=Learn+X+in+Y+minutes%2C+where+X%3DAsymptotic+Notation">
        Share this page
      </a></span>

      <span class='st_facebook_large' displayText='Facebook'></span>
      <span class='st_twitter_large' displayText='Tweet'></span>
  </div>
  <div class="theme-choice">
    <label id="theme-label">Select theme:</label>
    <button type="button" aria-labelledby="theme-label" onclick="set_theme('light');">light</button>
    <button type="button" aria-labelledby="theme-label" onclick="set_theme('dark');">dark</button>
  </div>
  <h1><a href="/">Learn X in Y minutes</a></h1>
  <h2>Where X=Asymptotic Notation</h2>
  <div id="doc">
    <h1>Asymptotic Notations</h1>

<h2>What are they?</h2>

<p>Asymptotic Notations are languages that allow us to analyze an algorithm&rsquo;s 
running time by identifying its behavior as the input size for the algorithm 
increases. This is also known as an algorithm&rsquo;s growth rate. Does the 
algorithm suddenly become incredibly slow when the input size grows? Does it 
mostly maintain its quick run time as the input size increases? Asymptotic 
Notation gives us the ability to answer these questions.</p>

<h2>Are there alternatives to answering these questions?</h2>

<p>One way would be to count the number of primitive operations at different 
input sizes. Though this is a valid solution, the amount of work this takes 
for even simple algorithms does not justify its use.</p>

<p>Another way is to physically measure the amount of time an algorithm takes to 
complete given different input sizes. However, the accuracy and relativity 
(times obtained would only be relative to the machine they were computed on) 
of this method is bound to environmental variables such as computer hardware 
specifications, processing power, etc.</p>

<h2>Types of Asymptotic Notation</h2>

<p>In the first section of this doc, we described how an Asymptotic Notation 
identifies the behavior of an algorithm as the input size changes. Let us 
imagine an algorithm as a function f, n as the input size, and f(n) being 
the running time. So for a given algorithm f, with input size n you get 
some resultant run time f(n). This results in a graph where the Y-axis is 
the runtime, the X-axis is the input size, and plot points are the resultants 
of the amount of time for a given input size.</p>

<p>You can label a function, or algorithm, with an Asymptotic Notation in many 
different ways. Some examples are, you can describe an algorithm by its best 
case, worst case, or average case. The most common is to analyze an algorithm 
by its worst case. You typically don’t evaluate by best case because those 
conditions aren’t what you’re planning for. An excellent example of this is 
sorting algorithms; particularly, adding elements to a tree structure. The 
best case for most algorithms could be as low as a single operation. However, 
in most cases, the element you’re adding needs to be sorted appropriately 
through the tree, which could mean examining an entire branch. This is 
the worst case, and this is what we plan for.</p>

<h3>Types of functions, limits, and simplification</h3>
<div class="highlight"><pre><span></span>Logarithmic Function - log n
Linear Function - an + b
Quadratic Function - an^2 + bn + c
Polynomial Function - an^z + . . . + an^2 + a*n^1 + a*n^0, where z is some 
constant
Exponential Function - a^n, where a is some constant
</pre></div>
<p>These are some fundamental function growth classifications used in 
various notations. The list starts at the slowest growing function 
(logarithmic, fastest execution time) and goes on to the fastest 
growing (exponential, slowest execution time). Notice that as ‘n’ 
or the input, increases in each of those functions, the result 
increases much quicker in quadratic, polynomial, and exponential, 
compared to logarithmic and linear.</p>

<p>It is worth noting that for the notations about to be discussed, 
you should do your best to use the simplest terms. This means to 
disregard constants, and lower order terms, because as the input 
size (or n in our f(n) example) increases to infinity (mathematical 
limits), the lower order terms and constants are of little to no 
importance. That being said, if you have constants that are 2^9001, 
or some other ridiculous, unimaginable amount, realize that 
simplifying skew your notation accuracy.</p>

<p>Since we want simplest form, lets modify our table a bit&hellip;</p>
<div class="highlight"><pre><span></span>Logarithmic - log n
Linear - n
Quadratic - n^2
Polynomial - n^z, where z is some constant
Exponential - a^n, where a is some constant
</pre></div>
<h3>Big-O</h3>

<p>Big-O, commonly written as <strong>O</strong>, is an Asymptotic Notation for the worst 
case, or ceiling of growth for a given function. It provides us with an 
<em><strong>asymptotic upper bound</strong></em> for the growth rate of the runtime of an algorithm.
Say <code>f(n)</code> is your algorithm runtime, and <code>g(n)</code> is an arbitrary time 
complexity you are trying to relate to your algorithm. <code>f(n)</code> is O(g(n)), if 
for some real constants c (c &gt; 0) and n<sub>0</sub>, <code>f(n)</code> &lt;= <code>c g(n)</code> for every input size 
n (n &gt; n<sub>0</sub>).</p>

<p><em>Example 1</em></p>
<div class="highlight"><pre><span></span>f(n) = 3log n + 100
g(n) = log n
</pre></div>
<p>Is <code>f(n)</code> O(g(n))?
Is <code>3 log n + 100</code> O(log n)?
Let&rsquo;s look to the definition of Big-O.</p>
<div class="highlight"><pre><span></span>3log n + 100 &lt;= c * log n
</pre></div>
<p>Is there some pair of constants c, n<sub>0</sub> that satisfies this for all n &gt; n<sub>0</sub>?</p>
<div class="highlight"><pre><span></span>3log n + 100 &lt;= 150 * log n, n &gt; 2 (undefined at n = 1)
</pre></div>
<p>Yes! The definition of Big-O has been met therefore <code>f(n)</code> is O(g(n)).</p>

<p><em>Example 2</em></p>
<div class="highlight"><pre><span></span>f(n) = 3*n^2
g(n) = n
</pre></div>
<p>Is <code>f(n)</code> O(g(n))?
Is <code>3 * n^2</code> O(n)?
Let&rsquo;s look at the definition of Big-O.</p>
<div class="highlight"><pre><span></span>3 * n^2 &lt;= c * n
</pre></div>
<p>Is there some pair of constants c, n<sub>0</sub> that satisfies this for all n &gt; n<sub>0</sub>?
No, there isn&rsquo;t. <code>f(n)</code> is NOT O(g(n)).</p>

<h3>Big-Omega</h3>

<p>Big-Omega, commonly written as <strong>Ω</strong>, is an Asymptotic Notation for the best 
case, or a floor growth rate for a given function. It provides us with an 
<em><strong>asymptotic lower bound</strong></em> for the growth rate of the runtime of an algorithm.</p>

<p><code>f(n)</code> is Ω(g(n)), if for some real constants c (c &gt; 0) and n<sub>0</sub> (n<sub>0</sub> &gt; 0), <code>f(n)</code> is &gt;= <code>c g(n)</code> 
for every input size n (n &gt; n<sub>0</sub>).</p>

<h3>Note</h3>

<p>The asymptotic growth rates provided by big-O and big-omega notation may or 
may not be asymptotically tight. Thus we use small-o and small-omega notation 
to denote bounds that are not asymptotically tight. </p>

<h3>Small-o</h3>

<p>Small-o, commonly written as <strong>o</strong>, is an Asymptotic Notation to denote the 
upper bound (that is not asymptotically tight) on the growth rate of runtime 
of an algorithm.</p>

<p><code>f(n)</code> is o(g(n)), if for all real constants c (c &gt; 0) and n<sub>0</sub> (n<sub>0</sub> &gt; 0), <code>f(n)</code> is &lt; <code>c g(n)</code> 
for every input size n (n &gt; n<sub>0</sub>).</p>

<p>The definitions of O-notation and o-notation are similar. The main difference 
is that in f(n) = O(g(n)), the bound f(n) &lt;= g(n) holds for <em><strong>some</strong></em> 
constant c &gt; 0, but in f(n) = o(g(n)), the bound f(n) &lt; c g(n) holds for 
<em><strong>all</strong></em> constants c &gt; 0.</p>

<h3>Small-omega</h3>

<p>Small-omega, commonly written as <strong>ω</strong>, is an Asymptotic Notation to denote 
the lower bound (that is not asymptotically tight) on the growth rate of 
runtime of an algorithm.</p>

<p><code>f(n)</code> is ω(g(n)), if for all real constants c (c &gt; 0) and n<sub>0</sub> (n<sub>0</sub> &gt; 0), <code>f(n)</code> is &gt; <code>c g(n)</code> 
for every input size n (n &gt; n<sub>0</sub>).</p>

<p>The definitions of Ω-notation and ω-notation are similar. The main difference 
is that in f(n) = Ω(g(n)), the bound f(n) &gt;= g(n) holds for <em><strong>some</strong></em> 
constant c &gt; 0, but in f(n) = ω(g(n)), the bound f(n) &gt; c g(n) holds for 
<em><strong>all</strong></em> constants c &gt; 0.</p>

<h3>Theta</h3>

<p>Theta, commonly written as <strong>Θ</strong>, is an Asymptotic Notation to denote the 
<em><strong>asymptotically tight bound</strong></em> on the growth rate of runtime of an algorithm. </p>

<p><code>f(n)</code> is Θ(g(n)), if for some real constants c1, c2 and n<sub>0</sub> (c1 &gt; 0, c2 &gt; 0, n<sub>0</sub> &gt; 0), 
<code>c1 g(n)</code> is &lt; <code>f(n)</code> is &lt; <code>c2 g(n)</code> for every input size n (n &gt; n<sub>0</sub>).</p>

<p>∴ <code>f(n)</code> is Θ(g(n)) implies <code>f(n)</code> is O(g(n)) as well as <code>f(n)</code> is Ω(g(n)).</p>

<p>Feel free to head over to additional resources for examples on this. Big-O 
is the primary notation use for general algorithm time complexity.</p>

<h3>Endnotes</h3>

<p>It&rsquo;s hard to keep this kind of topic short, and you should go 
through the books and online resources listed. They go into much greater depth 
with definitions and examples. More where x=&lsquo;Algorithms &amp; Data Structures&rsquo; is 
on its way; we&rsquo;ll have a doc up on analyzing actual code examples soon.</p>

<h2>Books</h2>

<ul>
<li><a href="http://www.amazon.com/Algorithms-4th-Robert-Sedgewick/dp/032157351X">Algorithms</a></li>
<li><a href="http://www.amazon.com/Algorithm-Design-Foundations-Analysis-Internet/dp/0471383651">Algorithm Design</a></li>
</ul>

<h2>Online Resources</h2>

<ul>
<li><a href="http://web.mit.edu/16.070/www/lecture/big_o.pdf">MIT</a></li>
<li><a href="https://www.khanacademy.org/computing/computer-science/algorithms/asymptotic-notation/a/asymptotic-notation">KhanAcademy</a></li>
<li><a href="http://bigocheatsheet.com/">Big-O Cheatsheet</a> - common structures, operations, and algorithms, ranked by complexity.</li>
</ul>

    <hr>
    <p>Got a suggestion? A correction, perhaps? <a href="https://github.com/adambard/learnxinyminutes-docs/issues/new">Open an Issue</a> on the Github Repo, or make a <a href="https://github.com/adambard/learnxinyminutes-docs/edit/master/asymptotic-notation.html.markdown">pull request</a> yourself!
    </p>
    <p class="contributed">
    Originally contributed by Jake Prather, and updated by <a href="https://github.com/adambard/learnxinyminutes-docs/blame/master/asymptotic-notation.html.markdown">9 contributors</a>.
    </p>

    <footer>
    <a style="float: left" rel="license" href="https://creativecommons.org/licenses/by-sa/3.0/deed.en_US"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/3.0/88x31.png" /></a>
    <p>
    &copy; 2024
        <a href="http://github.com/JakeHP">Jake Prather</a>,
        <a href="http://github.com/divayprakash">Divay Prakash</a>
    </p>

    <p>

    </footer>
  </div>

        </div>

        <script src="//ajax.googleapis.com/ajax/libs/jquery/1.8.0/jquery.min.js"></script>
        <script src="/js/script.js"></script>
    </body>
</html>
